{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run GNNExplainer on graph test/val splits for isoform 1A2, save PNG visualizations and CSV indexes\n",
    "\n",
    "Produces per-sample PNGs, per-split CSVs, master_index.csv, README.md and qc_summary.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math, json, csv, time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.explain import Explainer, Explanation\n",
    "from torch_geometric.explain.algorithm import GNNExplainer\n",
    "from torch_geometric.explain.config import ModelConfig\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem import rdMolDescriptors \n",
    "\n",
    "\n",
    "from model_GINE import GINEModel as GraphModelClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Config / paths \n",
    "# ------------------------------\n",
    "ISOFORM = \"1A2\"\n",
    "GRAPH_ROOT_BASE = os.path.join(\"..\", \"GraphDataset\", ISOFORM)\n",
    "CSV_BASE = os.path.join(\"..\", \"data\", \"processed\")\n",
    "CSV_TEST = os.path.join(CSV_BASE, f\"{ISOFORM}_test.csv\")\n",
    "CSV_VAL  = os.path.join(CSV_BASE, f\"{ISOFORM}_val.csv\")\n",
    "MODEL_PATH = f\"models/GINE_CYP{ISOFORM}.pth\"\n",
    "\n",
    "OUT_ROOT = os.path.join(\"..\", \"GNNExplainer\", ISOFORM)\n",
    "SPLITS = {\"Test\": os.path.join(GRAPH_ROOT_BASE, \"test\"),\n",
    "          \"Val\":  os.path.join(GRAPH_ROOT_BASE, \"val\")}\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Explainer params\n",
    "EXPLAINER_PARAMS = {\n",
    "    \"algorithm_epochs\": 100,\n",
    "    \"algorithm_lr\": 0.01,\n",
    "    \"node_mask_type\": \"object\",\n",
    "    \"edge_mask_type\": \"object\",\n",
    "    \"normalize_per_molecule\": True,\n",
    "    \"top_selection\": \"top_k_or_threshold\",  # reported in README\n",
    "    \"top_k\": 5,\n",
    "    \"threshold_frac\": 0.10,  # 10% of max\n",
    "}\n",
    "\n",
    "# CSV header template\n",
    "CSV_COLUMNS = [\n",
    "    \"Drug_ID\", \"Split\", \"Isoform\", \"True_Label\", \"Pred_Label\", \"Logit\", \"Prob\", \"PNG_Path\",\n",
    "    \"Mol_SMILES\", \"Num_Atoms\", \"Num_Bonds\", \"Num_Rings\"\n",
    "]\n",
    "# add Top_Atom_1..5 and Top_Bond_1..5\n",
    "for i in range(1, EXPLAINER_PARAMS[\"top_k\"] + 1):\n",
    "    CSV_COLUMNS.append(f\"Top_Atom_{i}\")\n",
    "for i in range(1, EXPLAINER_PARAMS[\"top_k\"] + 1):\n",
    "    CSV_COLUMNS.append(f\"Top_Bond_{i}\")\n",
    "# metadata fields\n",
    "CSV_COLUMNS += [\"Explainer_Params\", \"Model_Checkpoint\", \"Date\", \"Seed\", \"Notes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def load_graph_pt(pt_path):\n",
    "    return torch.load(pt_path)\n",
    "def drug_id_from_filename(fname: str) -> str:\n",
    "    name = Path(fname).stem\n",
    "    return name.split(\"_\")[0]\n",
    "def normalize_mask(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        return x\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    if math.isclose(mx, mn):\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "def ensure_dirs():\n",
    "    # create per-split and per-category dirs\n",
    "    categories = [\"True_Positive\", \"True_Negative\", \"False_Positive\", \"False_Negative\"]\n",
    "    for split in SPLITS.keys():\n",
    "        sroot = os.path.join(OUT_ROOT, split)\n",
    "        os.makedirs(sroot, exist_ok=True)\n",
    "        for cat in categories:\n",
    "            os.makedirs(os.path.join(sroot, cat), exist_ok=True)\n",
    "\n",
    "def rdkit_draw_and_save(mol: Chem.Mol, atom_scores: Optional[np.ndarray], bond_scores: Optional[np.ndarray],\n",
    "                        out_path: str, size=(600,600)) -> None:\n",
    "    \"\"\"Draw molecule with highlighted atoms/bonds and save PNG to out_path\"\"\"\n",
    "    if mol is None:\n",
    "        # create a blank placeholder\n",
    "        arr = np.zeros((size[1], size[0], 3), dtype=np.uint8)\n",
    "        plt.imsave(out_path, arr)\n",
    "        return\n",
    "\n",
    "    atom_scores_norm = None if atom_scores is None else normalize_mask(atom_scores).ravel()\n",
    "    bond_scores_norm = None if bond_scores is None else normalize_mask(bond_scores).ravel()\n",
    "\n",
    "    atom_colors = {}\n",
    "    if atom_scores_norm is not None and atom_scores_norm.size>0:\n",
    "        cmap = plt.get_cmap(\"OrRd\")\n",
    "        for i, v in enumerate(atom_scores_norm):\n",
    "            rgba = cmap(float(v))\n",
    "            atom_colors[i] = (float(rgba[0]), float(rgba[1]), float(rgba[2]))\n",
    "\n",
    "    bond_indices = []\n",
    "    bond_colors = {}\n",
    "    if bond_scores_norm is not None and bond_scores_norm.size>0:\n",
    "        cmap = plt.get_cmap(\"OrRd\")\n",
    "        # bond_scores aligned to edge_index order; mapping handled by caller\n",
    "        for bidx, v in enumerate(bond_scores_norm):\n",
    "            rgba = cmap(float(v))\n",
    "            bond_colors[bidx] = (float(rgba[0]), float(rgba[1]), float(rgba[2]))\n",
    "            bond_indices.append(bidx)\n",
    "\n",
    "    try:\n",
    "        drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1])\n",
    "        rdMolDraw2D.PrepareAndDrawMolecule(drawer, mol,\n",
    "                                           highlightAtoms=list(atom_colors.keys()),\n",
    "                                           highlightAtomColors={k: atom_colors[k] for k in atom_colors},\n",
    "                                           highlightBonds=bond_indices,\n",
    "                                           highlightBondColors={k: bond_colors[k] for k in bond_colors})\n",
    "        drawer.FinishDrawing()\n",
    "        png_bytes = drawer.GetDrawingText()\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(png_bytes)\n",
    "    except Exception as e:\n",
    "        print(\"RDKit draw failed for\", out_path, e)\n",
    "        # fallback: save empty image\n",
    "        arr = np.zeros((size[1], size[0], 3), dtype=np.uint8)\n",
    "        plt.imsave(out_path, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Main flow\n",
    "# ------------------------------\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "    # load optional SMILES maps\n",
    "    smiles_map = {}\n",
    "    for csvp, splitname in [(CSV_TEST, \"Test\"), (CSV_VAL, \"Val\")]:\n",
    "        if os.path.exists(csvp):\n",
    "            df = pd.read_csv(csvp)\n",
    "            id_col = \"Drug_ID\" if \"Drug_ID\" in df.columns else \"Drug\"\n",
    "            smi_col = \"Drug\" if \"Drug\" in df.columns else df.columns[-1]\n",
    "            for _, r in df.iterrows():\n",
    "                try:\n",
    "                    key = str(int(r[id_col]))\n",
    "                except Exception:\n",
    "                    key = str(r[id_col])\n",
    "                smiles_map[(splitname, key)] = str(r[smi_col])\n",
    "\n",
    "    # load model\n",
    "    print(\"Loading model from\", MODEL_PATH)\n",
    "    ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "\n",
    "    # need to construct model with sample dims; find any .pt to inspect dims\n",
    "    any_pt = None\n",
    "    for split, folder in SPLITS.items():\n",
    "        files = sorted(glob.glob(os.path.join(folder, \"*.pt\")))\n",
    "        if files:\n",
    "            any_pt = files[0]\n",
    "            break\n",
    "    if any_pt is None:\n",
    "        raise RuntimeError(\"No .pt graph files found in splits\")\n",
    "    sample = load_graph_pt(any_pt)\n",
    "    in_node_feats = sample.x.shape[1]\n",
    "    in_edge_feats = sample.edge_attr.shape[1] if hasattr(sample, \"edge_attr\") and sample.edge_attr is not None else None\n",
    "\n",
    "    model_params = {\n",
    "        \"model_embedding_size\": 128,\n",
    "        \"model_layers\": 4,\n",
    "        \"model_dropout_rate\": 0.2,\n",
    "        \"model_dense_neurons\": 256,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model = GraphModelClass(in_node_feats, in_edge_feats, model_params)\n",
    "    except TypeError:\n",
    "        model = GraphModelClass(in_node_feats, model_params)\n",
    "\n",
    "    # load weights tolerant\n",
    "    if isinstance(ckpt, dict) and (\"model\" in ckpt or any(k.startswith(\"module.\") for k in ckpt.keys())):\n",
    "        state = ckpt.get(\"model\", ckpt)\n",
    "    else:\n",
    "        state = ckpt\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # build explainer\n",
    "    algorithm = GNNExplainer(epochs=EXPLAINER_PARAMS[\"algorithm_epochs\"], lr=EXPLAINER_PARAMS[\"algorithm_lr\"]) \n",
    "    model_conf = ModelConfig(mode='binary_classification', task_level='graph', return_type='raw')\n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "        algorithm=algorithm,\n",
    "        explanation_type='model',\n",
    "        model_config=model_conf,\n",
    "        node_mask_type=EXPLAINER_PARAMS['node_mask_type'] if 'node_mask_type' in EXPLAINER_PARAMS else 'object',\n",
    "        edge_mask_type=EXPLAINER_PARAMS['edge_mask_type'] if 'edge_mask_type' in EXPLAINER_PARAMS else 'object'\n",
    "    )\n",
    "\n",
    "    master_rows = []\n",
    "\n",
    "    for split, folder in SPLITS.items():\n",
    "        print(\"Processing split:\", split, \"folder:\", folder)\n",
    "        pt_files = sorted(glob.glob(os.path.join(folder, \"*.pt\")))\n",
    "        csv_rows = []\n",
    "        qc = {\"TP\":0, \"TN\":0, \"FP\":0, \"FN\":0, \"images\":0, \"failures\":0}\n",
    "\n",
    "        # CSV path for this split\n",
    "        csv_out_path = os.path.join(OUT_ROOT, f\"index_{ISOFORM}_{split}.csv\")\n",
    "        # if exists, start fresh\n",
    "        with open(csv_out_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=CSV_COLUMNS)\n",
    "            writer.writeheader()\n",
    "\n",
    "        for pt in pt_files:\n",
    "            try:\n",
    "                data = load_graph_pt(pt)\n",
    "                x = data.x.to(DEVICE).float()\n",
    "                edge_index = data.edge_index.to(DEVICE)\n",
    "                edge_attr = data.edge_attr.to(DEVICE).float() if hasattr(data, 'edge_attr') and data.edge_attr is not None else None\n",
    "                batch = torch.zeros(x.size(0), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "                # forward\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        logit_t = model(x, edge_index, edge_attr, batch)\n",
    "                    except TypeError:\n",
    "                        try:\n",
    "                            logit_t = model(x, edge_index, batch)\n",
    "                        except Exception:\n",
    "                            logit_t = model(x, edge_index)\n",
    "                    if isinstance(logit_t, torch.Tensor):\n",
    "                        logit = float(logit_t.detach().cpu().numpy().ravel()[0])\n",
    "                    else:\n",
    "                        logit = float(logit_t)\n",
    "                    prob = 1.0 / (1.0 + math.exp(-logit))\n",
    "\n",
    "                # true label: try data.y, else infer from filename\n",
    "                true_label = None\n",
    "                if hasattr(data, 'y'):\n",
    "                    try:\n",
    "                        true_label = int(data.y.detach().cpu().numpy().ravel()[0])\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            true_label = int(data.y)\n",
    "                        except Exception:\n",
    "                            true_label = None\n",
    "                if true_label is None:\n",
    "                    # infer from filename after underscore\n",
    "                    fname = Path(pt).stem\n",
    "                    if '_' in fname:\n",
    "                        true_label = int(fname.split('_')[-1])\n",
    "                    else:\n",
    "                        true_label = -1\n",
    "\n",
    "                pred_label = 1 if prob >= 0.5 else 0\n",
    "\n",
    "                # Run explainer\n",
    "                explanation: Explanation = explainer(x=x, edge_index=edge_index, edge_attr=edge_attr, batch=batch)\n",
    "                node_mask = explanation.node_mask.detach().cpu().numpy().ravel() if explanation.node_mask is not None else np.zeros(x.size(0))\n",
    "                edge_mask = explanation.edge_mask.detach().cpu().numpy().ravel() if explanation.edge_mask is not None else np.zeros(edge_index.shape[1])\n",
    "\n",
    "                # get smiles if available\n",
    "                drug_id = drug_id_from_filename(pt)\n",
    "                smiles = None\n",
    "                if (split, drug_id) in smiles_map:\n",
    "                    smiles = smiles_map[(split, drug_id)]\n",
    "                elif os.path.exists(CSV_TEST):\n",
    "                    # try general CSV_TEST map\n",
    "                    try:\n",
    "                        df = pd.read_csv(CSV_TEST)\n",
    "                        row = df[df['Drug_ID'].astype(str) == drug_id]\n",
    "                        if not row.empty:\n",
    "                            smiles = str(row.iloc[0]['Drug'])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                mol = Chem.MolFromSmiles(smiles) if smiles is not None else None\n",
    "\n",
    "                # Map edge_index -> RDKit bond idx importance aggregation\n",
    "                # edge_index shape [2,E]; edges may be bidirectional: we map each edge column to a bond id if exists\n",
    "                bond_importances = {}\n",
    "                ei = edge_index.detach().cpu().numpy()\n",
    "                for col in range(ei.shape[1]):\n",
    "                    u, v = int(ei[0, col]), int(ei[1, col])\n",
    "                    if mol is not None:\n",
    "                        b = mol.GetBondBetweenAtoms(u, v)\n",
    "                        if b is not None:\n",
    "                            bidx = b.GetIdx()\n",
    "                            bond_importances.setdefault(bidx, []).append(float(edge_mask[col]))\n",
    "                        else:\n",
    "                            # keep placeholder for nonexistent bond, map to -1\n",
    "                            bond_importances.setdefault(-1, []).append(float(edge_mask[col]))\n",
    "                    else:\n",
    "                        # no mol -> keep per-edge mask as per-col\n",
    "                        bond_importances.setdefault(col, []).append(float(edge_mask[col]))\n",
    "\n",
    "                # aggregate bond importances: take max across directions if duplicates\n",
    "                bond_ids = sorted(list(bond_importances.keys()))\n",
    "                bond_id_vals = []\n",
    "                for b in bond_ids:\n",
    "                    vals = bond_importances[b]\n",
    "                    bond_id_vals.append((b, float(np.max(vals))))\n",
    "\n",
    "                # build arrays aligned to RDKit bond indices if mol present\n",
    "                num_atoms = mol.GetNumAtoms() if mol is not None else int(x.size(0))\n",
    "                num_bonds = mol.GetNumBonds() if mol is not None else int(len(bond_id_vals))\n",
    "\n",
    "                atom_scores = node_mask if node_mask is not None else np.zeros(num_atoms)\n",
    "                # build bond_scores_rdkit array where index == bond.GetIdx()\n",
    "                bond_scores_rd = np.zeros(num_bonds, dtype=float)\n",
    "                if mol is not None:\n",
    "                    for (bidx, val) in bond_id_vals:\n",
    "                        if bidx >= 0 and bidx < num_bonds:\n",
    "                            bond_scores_rd[bidx] = val\n",
    "                else:\n",
    "                    # when mol missing, try to create bond_scores based on bond_id_vals order\n",
    "                    for i, (k, v) in enumerate(bond_id_vals):\n",
    "                        if i < num_bonds:\n",
    "                            bond_scores_rd[i] = v\n",
    "\n",
    "                # Normalize both raw and normalized\n",
    "                atom_scores_raw = atom_scores.copy()\n",
    "                bond_scores_raw = bond_scores_rd.copy()\n",
    "                atom_scores_norm = normalize_mask(atom_scores_raw) if EXPLAINER_PARAMS['normalize_per_molecule'] else atom_scores_raw\n",
    "                bond_scores_norm = normalize_mask(bond_scores_raw) if EXPLAINER_PARAMS['normalize_per_molecule'] else bond_scores_raw\n",
    "\n",
    "                # pick top atoms and bonds\n",
    "                abs_atom = np.abs(atom_scores_raw)\n",
    "                top_atom_idxs = list(np.argsort(-abs_atom)[:EXPLAINER_PARAMS['top_k']])\n",
    "                top_atoms = []\n",
    "                for idx in top_atom_idxs:\n",
    "                    sym = mol.GetAtomWithIdx(int(idx)).GetSymbol() if mol is not None else \"\"\n",
    "                    top_atoms.append((int(idx), float(atom_scores_raw[int(idx)]), sym))\n",
    "\n",
    "                abs_bond = np.abs(bond_scores_raw)\n",
    "                top_bond_idxs = list(np.argsort(-abs_bond)[:EXPLAINER_PARAMS['top_k']])\n",
    "                top_bonds = []\n",
    "                # mapping from bond idx to atom i-j\n",
    "                if mol is not None:\n",
    "                    for bidx in top_bond_idxs:\n",
    "                        if bidx >=0 and bidx < mol.GetNumBonds():\n",
    "                            b = mol.GetBondWithIdx(int(bidx))\n",
    "                            ai = b.GetBeginAtomIdx(); aj = b.GetEndAtomIdx()\n",
    "                            top_bonds.append((int(bidx), float(bond_scores_raw[int(bidx)]), f\"{ai}-{aj}\"))\n",
    "                        else:\n",
    "                            top_bonds.append((int(bidx), float(bond_scores_raw[int(bidx)]), \"-\"))\n",
    "                else:\n",
    "                    for bpos in top_bond_idxs:\n",
    "                        top_bonds.append((int(bpos), float(bond_scores_raw[int(bpos)]), \"-\"))\n",
    "\n",
    "                # prepare png path\n",
    "                category = None\n",
    "                if true_label == 1 and pred_label == 1:\n",
    "                    category = \"True_Positive\"\n",
    "                    qc['TP'] += 1\n",
    "                elif true_label == 0 and pred_label == 0:\n",
    "                    category = \"True_Negative\"\n",
    "                    qc['TN'] += 1\n",
    "                elif true_label == 0 and pred_label == 1:\n",
    "                    category = \"False_Positive\"\n",
    "                    qc['FP'] += 1\n",
    "                elif true_label == 1 and pred_label == 0:\n",
    "                    category = \"False_Negative\"\n",
    "                    qc['FN'] += 1\n",
    "                else:\n",
    "                    category = \"Unk\"\n",
    "\n",
    "                stem = f\"{drug_id}__true{true_label}__pred{pred_label}__prob{prob:.4f}__logit{logit:.4f}\"\n",
    "                out_png = os.path.join(OUT_ROOT, split, category, stem + \".png\")\n",
    "\n",
    "                # draw and save\n",
    "                rdkit_draw_and_save(mol, atom_scores_norm, bond_scores_norm, out_png, size=(400,400))\n",
    "                qc['images'] += 1\n",
    "\n",
    "                # build CSV row\n",
    "                row = {\n",
    "                    \"Drug_ID\": drug_id,\n",
    "                    \"Split\": split,\n",
    "                    \"Isoform\": ISOFORM,\n",
    "                    \"True_Label\": int(true_label),\n",
    "                    \"Pred_Label\": int(pred_label),\n",
    "                    \"Logit\": float(logit),\n",
    "                    \"Prob\": float(prob),\n",
    "                    \"PNG_Path\": os.path.relpath(out_png),\n",
    "                    \"Mol_SMILES\": smiles if smiles is not None else \"\",\n",
    "                    \"Num_Atoms\": int(num_atoms),\n",
    "                    \"Num_Bonds\": int(num_bonds),\n",
    "                    \"Num_Rings\": int(Chem.rdMolDescriptors.CalcNumRings(mol)) if mol is not None else 0,\n",
    "                }\n",
    "                # top atoms/bonds into columns as strings\n",
    "                for i in range(EXPLAINER_PARAMS['top_k']):\n",
    "                    if i < len(top_atoms):\n",
    "                        a = top_atoms[i]\n",
    "                        row[f\"Top_Atom_{i+1}\"] = f\"{a[0]},{a[1]:.6f},{a[2]}\"\n",
    "                    else:\n",
    "                        row[f\"Top_Atom_{i+1}\"] = \"\"\n",
    "                for i in range(EXPLAINER_PARAMS['top_k']):\n",
    "                    if i < len(top_bonds):\n",
    "                        b = top_bonds[i]\n",
    "                        row[f\"Top_Bond_{i+1}\"] = f\"{b[0]},{b[1]:.6f},{b[2]}\"\n",
    "                    else:\n",
    "                        row[f\"Top_Bond_{i+1}\"] = \"\"\n",
    "\n",
    "                row[\"Explainer_Params\"] = json.dumps(EXPLAINER_PARAMS)\n",
    "                row[\"Model_Checkpoint\"] = MODEL_PATH\n",
    "                row[\"Date\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                row[\"Seed\"] = SEED\n",
    "                row[\"Notes\"] = \"\"\n",
    "\n",
    "                # append to split csv\n",
    "                with open(csv_out_path, 'a', newline='') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=CSV_COLUMNS)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "                master_rows.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                qc['failures'] += 1\n",
    "                print(\"Failure processing\", pt, e)\n",
    "\n",
    "        # write qc_summary\n",
    "        qc_path = os.path.join(OUT_ROOT, split, 'qc_summary.txt')\n",
    "        with open(qc_path, 'w') as f:\n",
    "            f.write(json.dumps(qc, indent=2))\n",
    "\n",
    "        print(f\"Finished split {split}. QC:\", qc)\n",
    "\n",
    "    # write master index\n",
    "    master_csv = os.path.join(OUT_ROOT, 'master_index.csv')\n",
    "    if master_rows:\n",
    "        pd.DataFrame(master_rows).to_csv(master_csv, index=False)\n",
    "\n",
    "    # write README\n",
    "    readme = os.path.join(OUT_ROOT, 'README.md')\n",
    "    with open(readme, 'w') as f:\n",
    "        f.write(f\"# GNNExplainer outputs for {ISOFORM}\\n\\n\")\n",
    "        f.write(f\"Model checkpoint: {MODEL_PATH}\\n\")\n",
    "        f.write(f\"Explainer params: {json.dumps(EXPLAINER_PARAMS)}\\n\")\n",
    "        f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Seed: {SEED}\\n\")\n",
    "        f.write(\"Normalization: per-molecule max->1.0 applied to atom/bond importances. Both raw and normalized are recorded in CSV columns (raw in Top_* values are raw importance).\\n\")\n",
    "        f.write(\"Top selection: top_k by absolute importance.\\n\")\n",
    "        f.write(\"Train split outputs (if present) are for debugging only and should not be used for claims about generalization.\\n\")\n",
    "\n",
    "    print(\"All done. Outputs under:\", OUT_ROOT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/GINE_CYP1A2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_20996\\4192447800.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_20996\\4197420585.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(pt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: Test folder: ..\\GraphDataset\\1A2\\test\n",
      "Finished split Test. QC: {'TP': 553, 'TN': 519, 'FP': 114, 'FN': 73, 'images': 1259, 'failures': 0}\n",
      "Processing split: Val folder: ..\\GraphDataset\\1A2\\val\n",
      "Finished split Val. QC: {'TP': 571, 'TN': 478, 'FP': 140, 'FN': 68, 'images': 1257, 'failures': 0}\n",
      "All done. Outputs under: ..\\GNNExplainer\\1A2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
