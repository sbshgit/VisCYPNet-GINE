{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Featurization pairwise/dense edges\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem, RDLogger\n",
    "from deepchem.feat import PagtnMolGraphFeaturizer\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Silence RDKit warnings for cleaner output\n",
    "RDLogger.DisableLog('rdApp.warning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer (DeepChem). \n",
    "featurizer = PagtnMolGraphFeaturizer(max_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYP3A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ISOFORM = \"3A4\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "CSV_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "OUT_ROOT = os.path.join(\"..\", \"GraphDataset\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: ensure edge_index has shape [2, num_edges] for PyG\n",
    "def to_edge_index_tensor(edge_index):\n",
    "    # edge_index may be (num_edges, 2) or (2, num_edges). Convert to LongTensor [2, E].\n",
    "    ei = torch.tensor(edge_index, dtype=torch.long)\n",
    "    if ei.dim() == 2 and ei.shape[0] == 2:\n",
    "        # already [2, E]\n",
    "        return ei.contiguous()\n",
    "    elif ei.dim() == 2 and ei.shape[1] == 2:\n",
    "        # [E, 2] -> transpose\n",
    "        return ei.t().contiguous()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected edge_index shape: {tuple(ei.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9862 molecules for 3A4 [train] -> saving to ..\\GraphDataset\\3A4\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9862/9862 [02:38<00:00, 62.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed train: saved graphs to ..\\GraphDataset\\3A4\\train\n",
      "\n",
      "Processing 1232 molecules for 3A4 [val] -> saving to ..\\GraphDataset\\3A4\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1232/1232 [00:43<00:00, 28.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed val: saved graphs to ..\\GraphDataset\\3A4\\val\n",
      "\n",
      "Processing 1234 molecules for 3A4 [test] -> saving to ..\\GraphDataset\\3A4\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1234/1234 [00:19<00:00, 63.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed test: saved graphs to ..\\GraphDataset\\3A4\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over splits\n",
    "for split in SPLITS:\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{ISOFORM}_{split}.csv\")\n",
    "    out_dir = os.path.join(OUT_ROOT, ISOFORM, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} molecules for {ISOFORM} [{split}] -> saving to {out_dir}\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row[\"Drug\"]\n",
    "        try:\n",
    "            drug_id = int(float(row[\"Drug_ID\"]))\n",
    "        except Exception:\n",
    "            drug_id = str(row[\"Drug_ID\"])\n",
    "        label = int(row[\"Y\"])\n",
    "\n",
    "        # parse SMILES\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[WARN] invalid SMILES at index {idx}: {smiles}\")\n",
    "            continue\n",
    "\n",
    "        # Use public API: featurize returns a list (even if one mol)\n",
    "        try:\n",
    "            feats = featurizer.featurize([mol])   # returns a list-like result\n",
    "            if len(feats) == 0:\n",
    "                print(f\"[WARN] featurizer returned empty for {drug_id}\")\n",
    "                continue\n",
    "            f = feats[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] featurizer failed for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # f.node_features, f.edge_index, f.edge_features are expected attributes\n",
    "        # convert to torch tensors and ensure correct shapes\n",
    "        try:\n",
    "            x = torch.tensor(f.node_features, dtype=torch.float)          # [N_nodes, node_feat_dim]\n",
    "            edge_attr = torch.tensor(f.edge_features, dtype=torch.float)  # [num_edges, edge_feat_dim]\n",
    "            edge_index = to_edge_index_tensor(f.edge_index)               # [2, num_edges]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Bad featurizer output for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build Data object; save label as float (for BCEWithLogitsLoss) but can be changed later\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        # Save file with integer ID: \"<drugid>_<label>.pt\"\n",
    "        fname = f\"{drug_id}_{label}.pt\"\n",
    "        torch.save(data, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"→ Completed {split}: saved graphs to {out_dir}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GraphDataset/3A4/train\\1001112_1.pt x: torch.Size([31, 94]) edge_index: torch.Size([2, 961]) edge_attr: torch.Size([961, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001133_1.pt x: torch.Size([30, 94]) edge_index: torch.Size([2, 900]) edge_attr: torch.Size([900, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001459_1.pt x: torch.Size([35, 94]) edge_index: torch.Size([2, 1225]) edge_attr: torch.Size([1225, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\100181_0.pt x: torch.Size([22, 94]) edge_index: torch.Size([2, 484]) edge_attr: torch.Size([484, 42]) y: tensor([0.])\n",
      "../GraphDataset/3A4/train\\100426_0.pt x: torch.Size([28, 94]) edge_index: torch.Size([2, 784]) edge_attr: torch.Size([784, 42]) y: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6640\\3314604780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(p)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "paths = glob(\"../GraphDataset/3A4/train/*.pt\")[:5]\n",
    "for p in paths:\n",
    "    d = torch.load(p)\n",
    "    print(p, \"x:\", d.x.shape, \"edge_index:\", d.edge_index.shape, \"edge_attr:\", d.edge_attr.shape, \"y:\", d.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: 31\n",
      "node feat dim: 94\n",
      "first node features: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n",
      "edge_index first 10 cols: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "edge_attr first 5: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0.]])\n",
      "label: tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6640\\1355238580.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n",
    "print(\"N nodes:\", d.x.shape[0])\n",
    "print(\"node feat dim:\", d.x.shape[1])\n",
    "print(\"first node features:\", d.x[0])\n",
    "print(\"edge_index first 10 cols:\", d.edge_index[:, :10])\n",
    "print(\"edge_attr first 5:\", d.edge_attr[:5])\n",
    "print(\"label:\", d.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYP1A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ISOFORM = \"1A2\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "CSV_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "OUT_ROOT = os.path.join(\"..\", \"GraphDataset\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: ensure edge_index has shape [2, num_edges] for PyG\n",
    "def to_edge_index_tensor(edge_index):\n",
    "    # edge_index may be (num_edges, 2) or (2, num_edges). Convert to LongTensor [2, E].\n",
    "    ei = torch.tensor(edge_index, dtype=torch.long)\n",
    "    if ei.dim() == 2 and ei.shape[0] == 2:\n",
    "        # already [2, E]\n",
    "        return ei.contiguous()\n",
    "    elif ei.dim() == 2 and ei.shape[1] == 2:\n",
    "        # [E, 2] -> transpose\n",
    "        return ei.t().contiguous()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected edge_index shape: {tuple(ei.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10063 molecules for 1A2 [train] -> saving to ..\\GraphDataset\\1A2\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10063/10063 [02:22<00:00, 70.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed train: saved graphs to ..\\GraphDataset\\1A2\\train\n",
      "\n",
      "Processing 1257 molecules for 1A2 [val] -> saving to ..\\GraphDataset\\1A2\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1257/1257 [00:37<00:00, 33.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed val: saved graphs to ..\\GraphDataset\\1A2\\val\n",
      "\n",
      "Processing 1259 molecules for 1A2 [test] -> saving to ..\\GraphDataset\\1A2\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1259/1259 [00:47<00:00, 26.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed test: saved graphs to ..\\GraphDataset\\1A2\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over splits\n",
    "for split in SPLITS:\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{ISOFORM}_{split}.csv\")\n",
    "    out_dir = os.path.join(OUT_ROOT, ISOFORM, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} molecules for {ISOFORM} [{split}] -> saving to {out_dir}\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row[\"Drug\"]\n",
    "        try:\n",
    "            drug_id = int(float(row[\"Drug_ID\"]))\n",
    "        except Exception:\n",
    "            drug_id = str(row[\"Drug_ID\"])\n",
    "        label = int(row[\"Y\"])\n",
    "\n",
    "        # parse SMILES\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[WARN] invalid SMILES at index {idx}: {smiles}\")\n",
    "            continue\n",
    "\n",
    "        # Use public API: featurize returns a list (even if one mol)\n",
    "        try:\n",
    "            feats = featurizer.featurize([mol])   # returns a list-like result\n",
    "            if len(feats) == 0:\n",
    "                print(f\"[WARN] featurizer returned empty for {drug_id}\")\n",
    "                continue\n",
    "            f = feats[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] featurizer failed for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # f.node_features, f.edge_index, f.edge_features are expected attributes\n",
    "        # convert to torch tensors and ensure correct shapes\n",
    "        try:\n",
    "            x = torch.tensor(f.node_features, dtype=torch.float)          # [N_nodes, node_feat_dim]\n",
    "            edge_attr = torch.tensor(f.edge_features, dtype=torch.float)  # [num_edges, edge_feat_dim]\n",
    "            edge_index = to_edge_index_tensor(f.edge_index)               # [2, num_edges]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Bad featurizer output for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build Data object; save label as float (for BCEWithLogitsLoss) but can be changed later\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        # Save file with integer ID: \"<drugid>_<label>.pt\"\n",
    "        fname = f\"{drug_id}_{label}.pt\"\n",
    "        torch.save(data, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"→ Completed {split}: saved graphs to {out_dir}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GraphDataset/3A4/train\\1001112_1.pt x: torch.Size([31, 94]) edge_index: torch.Size([2, 961]) edge_attr: torch.Size([961, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001133_1.pt x: torch.Size([30, 94]) edge_index: torch.Size([2, 900]) edge_attr: torch.Size([900, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001459_1.pt x: torch.Size([35, 94]) edge_index: torch.Size([2, 1225]) edge_attr: torch.Size([1225, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\100181_0.pt x: torch.Size([22, 94]) edge_index: torch.Size([2, 484]) edge_attr: torch.Size([484, 42]) y: tensor([0.])\n",
      "../GraphDataset/3A4/train\\100426_0.pt x: torch.Size([28, 94]) edge_index: torch.Size([2, 784]) edge_attr: torch.Size([784, 42]) y: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\3314604780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(p)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "paths = glob(\"../GraphDataset/3A4/train/*.pt\")[:5]\n",
    "for p in paths:\n",
    "    d = torch.load(p)\n",
    "    print(p, \"x:\", d.x.shape, \"edge_index:\", d.edge_index.shape, \"edge_attr:\", d.edge_attr.shape, \"y:\", d.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: 31\n",
      "node feat dim: 94\n",
      "first node features: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n",
      "edge_index first 10 cols: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "edge_attr first 5: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0.]])\n",
      "label: tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\1355238580.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n",
    "print(\"N nodes:\", d.x.shape[0])\n",
    "print(\"node feat dim:\", d.x.shape[1])\n",
    "print(\"first node features:\", d.x[0])\n",
    "print(\"edge_index first 10 cols:\", d.edge_index[:, :10])\n",
    "print(\"edge_attr first 5:\", d.edge_attr[:5])\n",
    "print(\"label:\", d.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYP2C9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ISOFORM = \"2C9\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "CSV_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "OUT_ROOT = os.path.join(\"..\", \"GraphDataset\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: ensure edge_index has shape [2, num_edges] for PyG\n",
    "def to_edge_index_tensor(edge_index):\n",
    "    # edge_index may be (num_edges, 2) or (2, num_edges). Convert to LongTensor [2, E].\n",
    "    ei = torch.tensor(edge_index, dtype=torch.long)\n",
    "    if ei.dim() == 2 and ei.shape[0] == 2:\n",
    "        # already [2, E]\n",
    "        return ei.contiguous()\n",
    "    elif ei.dim() == 2 and ei.shape[1] == 2:\n",
    "        # [E, 2] -> transpose\n",
    "        return ei.t().contiguous()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected edge_index shape: {tuple(ei.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9673 molecules for 2C9 [train] -> saving to ..\\GraphDataset\\2C9\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9673/9673 [02:26<00:00, 65.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed train: saved graphs to ..\\GraphDataset\\2C9\\train\n",
      "\n",
      "Processing 1209 molecules for 2C9 [val] -> saving to ..\\GraphDataset\\2C9\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [00:15<00:00, 75.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed val: saved graphs to ..\\GraphDataset\\2C9\\val\n",
      "\n",
      "Processing 1210 molecules for 2C9 [test] -> saving to ..\\GraphDataset\\2C9\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1210/1210 [00:18<00:00, 66.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed test: saved graphs to ..\\GraphDataset\\2C9\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over splits\n",
    "for split in SPLITS:\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{ISOFORM}_{split}.csv\")\n",
    "    out_dir = os.path.join(OUT_ROOT, ISOFORM, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} molecules for {ISOFORM} [{split}] -> saving to {out_dir}\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row[\"Drug\"]\n",
    "        try:\n",
    "            drug_id = int(float(row[\"Drug_ID\"]))\n",
    "        except Exception:\n",
    "            drug_id = str(row[\"Drug_ID\"])\n",
    "        label = int(row[\"Y\"])\n",
    "\n",
    "        # parse SMILES\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[WARN] invalid SMILES at index {idx}: {smiles}\")\n",
    "            continue\n",
    "\n",
    "        # Use public API: featurize returns a list (even if one mol)\n",
    "        try:\n",
    "            feats = featurizer.featurize([mol])   # returns a list-like result\n",
    "            if len(feats) == 0:\n",
    "                print(f\"[WARN] featurizer returned empty for {drug_id}\")\n",
    "                continue\n",
    "            f = feats[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] featurizer failed for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # f.node_features, f.edge_index, f.edge_features are expected attributes\n",
    "        # convert to torch tensors and ensure correct shapes\n",
    "        try:\n",
    "            x = torch.tensor(f.node_features, dtype=torch.float)          # [N_nodes, node_feat_dim]\n",
    "            edge_attr = torch.tensor(f.edge_features, dtype=torch.float)  # [num_edges, edge_feat_dim]\n",
    "            edge_index = to_edge_index_tensor(f.edge_index)               # [2, num_edges]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Bad featurizer output for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build Data object; save label as float (for BCEWithLogitsLoss) but can be changed later\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        # Save file with integer ID: \"<drugid>_<label>.pt\"\n",
    "        fname = f\"{drug_id}_{label}.pt\"\n",
    "        torch.save(data, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"→ Completed {split}: saved graphs to {out_dir}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GraphDataset/3A4/train\\1001112_1.pt x: torch.Size([31, 94]) edge_index: torch.Size([2, 961]) edge_attr: torch.Size([961, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001133_1.pt x: torch.Size([30, 94]) edge_index: torch.Size([2, 900]) edge_attr: torch.Size([900, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001459_1.pt x: torch.Size([35, 94]) edge_index: torch.Size([2, 1225]) edge_attr: torch.Size([1225, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\100181_0.pt x: torch.Size([22, 94]) edge_index: torch.Size([2, 484]) edge_attr: torch.Size([484, 42]) y: tensor([0.])\n",
      "../GraphDataset/3A4/train\\100426_0.pt x: torch.Size([28, 94]) edge_index: torch.Size([2, 784]) edge_attr: torch.Size([784, 42]) y: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\3314604780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(p)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "paths = glob(\"../GraphDataset/3A4/train/*.pt\")[:5]\n",
    "for p in paths:\n",
    "    d = torch.load(p)\n",
    "    print(p, \"x:\", d.x.shape, \"edge_index:\", d.edge_index.shape, \"edge_attr:\", d.edge_attr.shape, \"y:\", d.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: 31\n",
      "node feat dim: 94\n",
      "first node features: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n",
      "edge_index first 10 cols: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "edge_attr first 5: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0.]])\n",
      "label: tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\1355238580.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n",
    "print(\"N nodes:\", d.x.shape[0])\n",
    "print(\"node feat dim:\", d.x.shape[1])\n",
    "print(\"first node features:\", d.x[0])\n",
    "print(\"edge_index first 10 cols:\", d.edge_index[:, :10])\n",
    "print(\"edge_attr first 5:\", d.edge_attr[:5])\n",
    "print(\"label:\", d.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2C9_train_downsampled.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ISOFORM = \"2C9\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "CSV_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "OUT_ROOT = os.path.join(\"..\", \"GraphDataset\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: ensure edge_index has shape [2, num_edges] for PyG\n",
    "def to_edge_index_tensor(edge_index):\n",
    "    # edge_index may be (num_edges, 2) or (2, num_edges). Convert to LongTensor [2, E].\n",
    "    ei = torch.tensor(edge_index, dtype=torch.long)\n",
    "    if ei.dim() == 2 and ei.shape[0] == 2:\n",
    "        # already [2, E]\n",
    "        return ei.contiguous()\n",
    "    elif ei.dim() == 2 and ei.shape[1] == 2:\n",
    "        # [E, 2] -> transpose\n",
    "        return ei.t().contiguous()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected edge_index shape: {tuple(ei.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6550 molecules for 2C9 [train] -> saving to ..\\GraphDataset\\2C9\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6550/6550 [03:03<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed train: saved graphs to ..\\GraphDataset\\2C9\\train\n",
      "\n",
      "Processing 1209 molecules for 2C9 [val] -> saving to ..\\GraphDataset\\2C9\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [00:24<00:00, 50.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed val: saved graphs to ..\\GraphDataset\\2C9\\val\n",
      "\n",
      "Processing 1210 molecules for 2C9 [test] -> saving to ..\\GraphDataset\\2C9\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1210/1210 [00:52<00:00, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed test: saved graphs to ..\\GraphDataset\\2C9\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over splits\n",
    "for split in SPLITS:\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{ISOFORM}_{split}.csv\")\n",
    "    out_dir = os.path.join(OUT_ROOT, ISOFORM, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} molecules for {ISOFORM} [{split}] -> saving to {out_dir}\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row[\"Drug\"]\n",
    "        try:\n",
    "            drug_id = int(float(row[\"Drug_ID\"]))\n",
    "        except Exception:\n",
    "            drug_id = str(row[\"Drug_ID\"])\n",
    "        label = int(row[\"Y\"])\n",
    "\n",
    "        # parse SMILES\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[WARN] invalid SMILES at index {idx}: {smiles}\")\n",
    "            continue\n",
    "\n",
    "        # Use public API: featurize returns a list (even if one mol)\n",
    "        try:\n",
    "            feats = featurizer.featurize([mol])   # returns a list-like result\n",
    "            if len(feats) == 0:\n",
    "                print(f\"[WARN] featurizer returned empty for {drug_id}\")\n",
    "                continue\n",
    "            f = feats[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] featurizer failed for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # f.node_features, f.edge_index, f.edge_features are expected attributes\n",
    "        # convert to torch tensors and ensure correct shapes\n",
    "        try:\n",
    "            x = torch.tensor(f.node_features, dtype=torch.float)          # [N_nodes, node_feat_dim]\n",
    "            edge_attr = torch.tensor(f.edge_features, dtype=torch.float)  # [num_edges, edge_feat_dim]\n",
    "            edge_index = to_edge_index_tensor(f.edge_index)               # [2, num_edges]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Bad featurizer output for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build Data object; save label as float (for BCEWithLogitsLoss) but can be changed later\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        # Save file with integer ID: \"<drugid>_<label>.pt\"\n",
    "        fname = f\"{drug_id}_{label}.pt\"\n",
    "        torch.save(data, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"→ Completed {split}: saved graphs to {out_dir}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GraphDataset/3A4/train\\1001112_1.pt x: torch.Size([31, 94]) edge_index: torch.Size([2, 961]) edge_attr: torch.Size([961, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001133_1.pt x: torch.Size([30, 94]) edge_index: torch.Size([2, 900]) edge_attr: torch.Size([900, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001459_1.pt x: torch.Size([35, 94]) edge_index: torch.Size([2, 1225]) edge_attr: torch.Size([1225, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\100181_0.pt x: torch.Size([22, 94]) edge_index: torch.Size([2, 484]) edge_attr: torch.Size([484, 42]) y: tensor([0.])\n",
      "../GraphDataset/3A4/train\\100426_0.pt x: torch.Size([28, 94]) edge_index: torch.Size([2, 784]) edge_attr: torch.Size([784, 42]) y: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_12236\\3314604780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(p)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "paths = glob(\"../GraphDataset/3A4/train/*.pt\")[:5]\n",
    "for p in paths:\n",
    "    d = torch.load(p)\n",
    "    print(p, \"x:\", d.x.shape, \"edge_index:\", d.edge_index.shape, \"edge_attr:\", d.edge_attr.shape, \"y:\", d.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: 31\n",
      "node feat dim: 94\n",
      "first node features: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n",
      "edge_index first 10 cols: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "edge_attr first 5: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0.]])\n",
      "label: tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_12236\\1355238580.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n",
    "print(\"N nodes:\", d.x.shape[0])\n",
    "print(\"node feat dim:\", d.x.shape[1])\n",
    "print(\"first node features:\", d.x[0])\n",
    "print(\"edge_index first 10 cols:\", d.edge_index[:, :10])\n",
    "print(\"edge_attr first 5:\", d.edge_attr[:5])\n",
    "print(\"label:\", d.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYP2C19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ISOFORM = \"2C19\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "CSV_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "OUT_ROOT = os.path.join(\"..\", \"GraphDataset\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: ensure edge_index has shape [2, num_edges] for PyG\n",
    "def to_edge_index_tensor(edge_index):\n",
    "    # edge_index may be (num_edges, 2) or (2, num_edges). Convert to LongTensor [2, E].\n",
    "    ei = torch.tensor(edge_index, dtype=torch.long)\n",
    "    if ei.dim() == 2 and ei.shape[0] == 2:\n",
    "        # already [2, E]\n",
    "        return ei.contiguous()\n",
    "    elif ei.dim() == 2 and ei.shape[1] == 2:\n",
    "        # [E, 2] -> transpose\n",
    "        return ei.t().contiguous()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected edge_index shape: {tuple(ei.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10132 molecules for 2C19 [train] -> saving to ..\\GraphDataset\\2C19\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10132/10132 [02:28<00:00, 68.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed train: saved graphs to ..\\GraphDataset\\2C19\\train\n",
      "\n",
      "Processing 1266 molecules for 2C19 [val] -> saving to ..\\GraphDataset\\2C19\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1266/1266 [00:19<00:00, 66.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed val: saved graphs to ..\\GraphDataset\\2C19\\val\n",
      "\n",
      "Processing 1267 molecules for 2C19 [test] -> saving to ..\\GraphDataset\\2C19\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1267/1267 [00:19<00:00, 64.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed test: saved graphs to ..\\GraphDataset\\2C19\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over splits\n",
    "for split in SPLITS:\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{ISOFORM}_{split}.csv\")\n",
    "    out_dir = os.path.join(OUT_ROOT, ISOFORM, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} molecules for {ISOFORM} [{split}] -> saving to {out_dir}\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row[\"Drug\"]\n",
    "        try:\n",
    "            drug_id = int(float(row[\"Drug_ID\"]))\n",
    "        except Exception:\n",
    "            drug_id = str(row[\"Drug_ID\"])\n",
    "\n",
    "        label = int(row[\"Y\"])\n",
    "\n",
    "        # parse SMILES\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[WARN] invalid SMILES at index {idx}: {smiles}\")\n",
    "            continue\n",
    "\n",
    "        # Use public API: featurize returns a list (even if one mol)\n",
    "        try:\n",
    "            feats = featurizer.featurize([mol])   # returns a list-like result\n",
    "            if len(feats) == 0:\n",
    "                print(f\"[WARN] featurizer returned empty for {drug_id}\")\n",
    "                continue\n",
    "            f = feats[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] featurizer failed for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # f.node_features, f.edge_index, f.edge_features are expected attributes\n",
    "        # convert to torch tensors and ensure correct shapes\n",
    "        try:\n",
    "            x = torch.tensor(f.node_features, dtype=torch.float)          # [N_nodes, node_feat_dim]\n",
    "            edge_attr = torch.tensor(f.edge_features, dtype=torch.float)  # [num_edges, edge_feat_dim]\n",
    "            edge_index = to_edge_index_tensor(f.edge_index)               # [2, num_edges]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Bad featurizer output for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build Data object; save label as float (for BCEWithLogitsLoss) but can be changed later\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        # Save file with integer ID: \"<drugid>_<label>.pt\"\n",
    "        fname = f\"{drug_id}_{label}.pt\"\n",
    "        torch.save(data, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"→ Completed {split}: saved graphs to {out_dir}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GraphDataset/3A4/train\\1001112_1.pt x: torch.Size([31, 94]) edge_index: torch.Size([2, 961]) edge_attr: torch.Size([961, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001133_1.pt x: torch.Size([30, 94]) edge_index: torch.Size([2, 900]) edge_attr: torch.Size([900, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001459_1.pt x: torch.Size([35, 94]) edge_index: torch.Size([2, 1225]) edge_attr: torch.Size([1225, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\100181_0.pt x: torch.Size([22, 94]) edge_index: torch.Size([2, 484]) edge_attr: torch.Size([484, 42]) y: tensor([0.])\n",
      "../GraphDataset/3A4/train\\100426_0.pt x: torch.Size([28, 94]) edge_index: torch.Size([2, 784]) edge_attr: torch.Size([784, 42]) y: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\3314604780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(p)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "paths = glob(\"../GraphDataset/3A4/train/*.pt\")[:5]\n",
    "for p in paths:\n",
    "    d = torch.load(p)\n",
    "    print(p, \"x:\", d.x.shape, \"edge_index:\", d.edge_index.shape, \"edge_attr:\", d.edge_attr.shape, \"y:\", d.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: 31\n",
      "node feat dim: 94\n",
      "first node features: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n",
      "edge_index first 10 cols: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "edge_attr first 5: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0.]])\n",
      "label: tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\1355238580.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n",
    "print(\"N nodes:\", d.x.shape[0])\n",
    "print(\"node feat dim:\", d.x.shape[1])\n",
    "print(\"first node features:\", d.x[0])\n",
    "print(\"edge_index first 10 cols:\", d.edge_index[:, :10])\n",
    "print(\"edge_attr first 5:\", d.edge_attr[:5])\n",
    "print(\"label:\", d.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYP2D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ISOFORM = \"2D6\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "CSV_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "OUT_ROOT = os.path.join(\"..\", \"GraphDataset\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: ensure edge_index has shape [2, num_edges] for PyG\n",
    "def to_edge_index_tensor(edge_index):\n",
    "    # edge_index may be (num_edges, 2) or (2, num_edges). Convert to LongTensor [2, E].\n",
    "    ei = torch.tensor(edge_index, dtype=torch.long)\n",
    "    if ei.dim() == 2 and ei.shape[0] == 2:\n",
    "        # already [2, E]\n",
    "        return ei.contiguous()\n",
    "    elif ei.dim() == 2 and ei.shape[1] == 2:\n",
    "        # [E, 2] -> transpose\n",
    "        return ei.t().contiguous()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected edge_index shape: {tuple(ei.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8176 molecules for 2D6 [train] -> saving to ..\\GraphDataset\\2D6\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8176/8176 [04:41<00:00, 29.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed train: saved graphs to ..\\GraphDataset\\2D6\\train\n",
      "\n",
      "Processing 1022 molecules for 2D6 [val] -> saving to ..\\GraphDataset\\2D6\\val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1022/1022 [00:36<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed val: saved graphs to ..\\GraphDataset\\2D6\\val\n",
      "\n",
      "Processing 1022 molecules for 2D6 [test] -> saving to ..\\GraphDataset\\2D6\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1022/1022 [00:34<00:00, 29.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Completed test: saved graphs to ..\\GraphDataset\\2D6\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over splits\n",
    "for split in SPLITS:\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{ISOFORM}_{split}.csv\")\n",
    "    out_dir = os.path.join(OUT_ROOT, ISOFORM, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} molecules for {ISOFORM} [{split}] -> saving to {out_dir}\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        smiles = row[\"Drug\"]\n",
    "        try:\n",
    "            drug_id = int(float(row[\"Drug_ID\"]))\n",
    "        except Exception:\n",
    "            drug_id = str(row[\"Drug_ID\"])\n",
    "        label = int(row[\"Y\"])\n",
    "\n",
    "        # parse SMILES\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[WARN] invalid SMILES at index {idx}: {smiles}\")\n",
    "            continue\n",
    "\n",
    "        # Use public API: featurize returns a list (even if one mol)\n",
    "        try:\n",
    "            feats = featurizer.featurize([mol])   # returns a list-like result\n",
    "            if len(feats) == 0:\n",
    "                print(f\"[WARN] featurizer returned empty for {drug_id}\")\n",
    "                continue\n",
    "            f = feats[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] featurizer failed for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # f.node_features, f.edge_index, f.edge_features are expected attributes\n",
    "        # convert to torch tensors and ensure correct shapes\n",
    "        try:\n",
    "            x = torch.tensor(f.node_features, dtype=torch.float)          # [N_nodes, node_feat_dim]\n",
    "            edge_attr = torch.tensor(f.edge_features, dtype=torch.float)  # [num_edges, edge_feat_dim]\n",
    "            edge_index = to_edge_index_tensor(f.edge_index)               # [2, num_edges]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Bad featurizer output for {drug_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build Data object; save label as float (for BCEWithLogitsLoss) but can be changed later\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor([label], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        # Save file with integer ID: \"<drugid>_<label>.pt\"\n",
    "        fname = f\"{drug_id}_{label}.pt\"\n",
    "        torch.save(data, os.path.join(out_dir, fname))\n",
    "\n",
    "    print(f\"→ Completed {split}: saved graphs to {out_dir}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../GraphDataset/3A4/train\\1001112_1.pt x: torch.Size([31, 94]) edge_index: torch.Size([2, 961]) edge_attr: torch.Size([961, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001133_1.pt x: torch.Size([30, 94]) edge_index: torch.Size([2, 900]) edge_attr: torch.Size([900, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\1001459_1.pt x: torch.Size([35, 94]) edge_index: torch.Size([2, 1225]) edge_attr: torch.Size([1225, 42]) y: tensor([1.])\n",
      "../GraphDataset/3A4/train\\100181_0.pt x: torch.Size([22, 94]) edge_index: torch.Size([2, 484]) edge_attr: torch.Size([484, 42]) y: tensor([0.])\n",
      "../GraphDataset/3A4/train\\100426_0.pt x: torch.Size([28, 94]) edge_index: torch.Size([2, 784]) edge_attr: torch.Size([784, 42]) y: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\3314604780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(p)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "paths = glob(\"../GraphDataset/3A4/train/*.pt\")[:5]\n",
    "for p in paths:\n",
    "    d = torch.load(p)\n",
    "    print(p, \"x:\", d.x.shape, \"edge_index:\", d.edge_index.shape, \"edge_attr:\", d.edge_attr.shape, \"y:\", d.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nodes: 31\n",
      "node feat dim: 94\n",
      "first node features: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n",
      "edge_index first 10 cols: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "edge_attr first 5: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0.]])\n",
      "label: tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afra\\AppData\\Local\\Temp\\ipykernel_6868\\1355238580.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.load(\"../GraphDataset/3A4/train/1001112_1.pt\")\n",
    "print(\"N nodes:\", d.x.shape[0])\n",
    "print(\"node feat dim:\", d.x.shape[1])\n",
    "print(\"first node features:\", d.x[0])\n",
    "print(\"edge_index first 10 cols:\", d.edge_index[:, :10])\n",
    "print(\"edge_attr first 5:\", d.edge_attr[:5])\n",
    "print(\"label:\", d.y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
